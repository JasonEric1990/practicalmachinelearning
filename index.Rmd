---
title: "Pratical machine learning coursera"
author: "eric"
date: "2016年3月5日"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<h1 class="title">Final Project Report - Practical Machine Learning Course</h1>
<h2>Link the file directly</h2>
```{r}
library(caret)
library(rpart)
library(data.table)
```{r}
url="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
url2="https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
```

```{r}
training.data=read.csv(url,header = TRUE)
test.data=read.csv(url2,header = TRUE)
```

Depending on the background,we want to use other covariate(especially the exercise' quantity and qualitative data)to predict the performance not to depend on person or times ,etc.And I also check whether the anticipant are the same or not,preventing the bias that will made.

<h2>Exploratory</h2>
just to see the person are same or not
```{r}
str(training.data)
```

We can see it contains a lot of NA

```{r}
levels(training.data$user_name)
levels(test.data$user_name)
```
<h2>Cleaning data</h2>
clean out the other unuseful variate
```{r}
training.data=training.data[,-(1:5)]
test.data=test.data[,-(1:5)]
```
clean the NAs that over the 0.8 of observation
```{r}
training.data=training.data[!colSums(is.na(training.data))>nrow(training.data)*0.8]
test.data=test.data[!colSums(is.na(test.data))>nrow(test.data)*0.8]
```
Find the near zero var and clean it.
```{r}
head(nearZeroVar(training.data, saveMetrics=TRUE))
head(nearZeroVar(test.data, saveMetrics=TRUE))
training.nzv=training.data[!nearZeroVar(training.data, saveMetrics=TRUE)$nzv==TRUE]
test.nzv=test.data[!nearZeroVar(test.data, saveMetrics=TRUE)$nzv==TRUE]
dim(training.nzv)
dim(test.nzv)
``````
check out if the lastest variate is the same or not
``````{r}
lookifthesame=ifelse(colnames(test.nzv)==colnames(training.nzv),"T","F")
lookifthesame
``````
Only the last one becasue the last one is the column we need to answer the predict result.

<h2>Dataspliting</h2>

Because the test data is really small and we don't know the result(out of box).so I need to do the cross validation to find the best accurary also reduce the rate of overfitting.

Split the training data into 0.9 subtraining and 0.1 subtesting ,after that I split the subtraining to 10 cross validation .Finally the output is __0.81 of subtraing each Chunk__
__(All depend by the training data,and I only use the subtraining set to train the model.Finally use the subtesting set to test the out of sample error).__
``````{r}
library(doMC)
registerDoMC(cores=4) ##make it faster by increasing thread
set.seed(1234)
trainIndex=createDataPartition(y=training.nzv$classe,p=0.9,list=FALSE)
subtraining=training.nzv[trainIndex,]
subtesting=training.nzv[-trainIndex,]
set.seed(1234)
flods=createFolds(y=subtraining$classe,list=TRUE,k=10,returnTrain=TRUE)
summary(flods)
``````
<h2>Modeling</h2>
use the trainControl to set the parameter
``````{r}
control <- trainControl(method = "cv", number = 10)
``````{r}
registerDoMC(cores=4)
set.seed(1234)
modfit.rp1=train(classe~.,method="rpart",trControl=control ,data=subtraining)
predrp1=predict(modfit.rp1,subtraining)
confusionMatrix(predrp1,subtraining$classe)$overall
``````
The accuracy is really bad,I add some preProces parameter to see if it perform better
``````{r}
registerDoMC(cores=4)
set.seed(1234)
modfit.rp2=train(classe~.,method="rpart",preProcess=c("scale","center"),trControl=control ,data=subtraining)
predrp2=predict(modfit.rp1,subtraining)
confusionMatrix(predrp2,subtraining$classe)
``````
seem the rpart's performance is still not good,so I use gbm to test
``````{r}
registerDoMC(cores=4)
set.seed(1234)
modfit.gbm=train(classe~.,method="gbm",trControl=control ,data=subtraining,verbose=FALSE)
predgbm=predict(modfit.gbm,subtraining)
confusionMatrix(predgbm,subtraining$classe)
``````
It’s really good.I use an other method to see it will be better or the same(I don’t use random forest because it’s really slow to my computer). I use svm to check out(No tuning the parameters.But if both the performances are equal I’ll try)
```````{r}
registerDoMC(cores=4)
library(e1071)
set.seed(1234)
modfit.svm=svm(classe~.,trControl=control ,data=subtraining)
predsvm=predict(modfit.svm,subtraining)
confusionMatrix(predsvm,subtraining$classe)
```````
Gbm seem better than Svm.I use subtesting to check out of sample error of both model
```````{r}
predgbm=predict(modfit.gbm,subtesting)
predsvm=predict(modfit.svm,subtesting)
confusionMatrix(predgbm,subtesting$classe)$overall
confusionMatrix(predsvm,subtesting$classe)$overall
oose.gbm <- 1 - as.numeric(confusionMatrix(subtesting$classe, predgbm)$overall[1])
oose.gbm 
oose.svm <- 1 - as.numeric(confusionMatrix(subtesting$classe, predsvm)$overall[1])
oose.svm
```````
Gbm is still better than Svm,and just have __less than 0.01 out of sample error.__

<h2>Appendix:Variate importance and some conclusion</h2>
Then I look the variance important of gbm(firt ten)
```````{r}
varimp=varImp(modfit.gbm, scale = FALSE)
```````
Then we look closer
```````{r}
plot(varimp,top=10)
```````

We can see the more detail of variance importance

I also make some relation’s plot of two most important variates to find out some evidence if it can explain

*I found that take the the log of num_window can be easier to understand the data so I take the log of it.

```````{r}
ggplot(training.nzv,aes(x=log(num_window),fill=classe))+geom_histogram()+facet_grid(classe~.)
ggplot(training.nzv,aes(x=roll_belt,fill=classe))+geom_histogram(binwidth = 0.1)+facet_grid(classe~.)
```````````


As you can see the distribution of two most important variate is different among classe(Not very significant but still can figure out,especially the E level compare to other levels).
<h2>The Prediction quiz</h2>
```````````{r}
pred.reult=predict(modfit.gbm,test.nzv)
pred.reult
````````````


